# -*- coding: utf-8 -*-
"""Project Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IFEdCzMnp5jcZMmPtbGg9EANSjaYGP8r
"""

import pandas as pd

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("KFUPM-JRCAI/arabic-generated-abstracts")

print(dataset)

df1 = pd.DataFrame(dataset['by_polishing'])

df2 = pd.DataFrame(dataset['from_title'])

df3 = pd.DataFrame(dataset['from_title_and_content'])

df = pd.concat([df1, df2, df3], ignore_index=True)

print(df.info())

print(df.columns)

print(df.dtypes)

print(df)

print(df.head(10))

print(df.head(100))

print(df.columns)

Human_df = pd.DataFrame({'abstract': df['original_abstract'],'label': 'Human'})

AI_abstracts = pd.concat([
    df['allam_generated_abstract'],df['jais_generated_abstract'],df['llama_generated_abstract'],df['openai_generated_abstract']], ignore_index=True)
AI_df = pd.DataFrame({
    'abstract': AI_abstracts,'label': 'AI'})

combined_df = pd.concat([Human_df, AI_df], ignore_index=True)

print(combined_df['label'].value_counts())

print(df.duplicated(['original_abstract']).sum())
print(df.duplicated(['allam_generated_abstract']).sum())
print(df.duplicated(['jais_generated_abstract']).sum())
print(df.duplicated(['llama_generated_abstract']).sum())
print(df.duplicated(['openai_generated_abstract']).sum())

print(df.isnull().sum())

print(df.apply(lambda x: str(x).strip()))



#phase2.1

import re
import nltk
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem.isri import ISRIStemmer
from datasets import load_dataset

#phase2.1

nltk.download('stopwords')

#phase2.1

print(df.head())

#phase2.1 #1

def normalize_arabic(text):
    text = re.sub("[Ø¥Ø£Ø¢Ø§]", "Ø§", text)
    text = re.sub("Ù‰", "ÙŠ", text)
    text = re.sub("Ø¤", "Ùˆ", text)
    text = re.sub("Ø¦", "ÙŠ", text)
    text = re.sub("Ø©", "Ù‡", text)
    text = re.sub("[^Ø€-Û¿ ]+", " ", text)
    return text

#phase2.1 #2

def remove_diacritics(text):
    arabic_diacritics = re.compile(r'[\u0617-\u061A\u064B-\u0652]')
    return re.sub(arabic_diacritics, '', text)

#Phase2.1 #3/4

arabic_stopwords = set(stopwords.words("arabic"))
stemmer = ISRIStemmer()

#Phase2.1

def preprocess_text(text):
    text = str(text)
    text = remove_diacritics(text)
    text = normalize_arabic(text)
    tokens = text.split()
    tokens = [w for w in tokens if w not in arabic_stopwords]
    tokens = [stemmer.stem(w) for w in tokens]
    return " ".join(tokens)

#Phase2.1

text_columns = [
    'original_abstract',
    'allam_generated_abstract',
    'jais_generated_abstract',
    'llama_generated_abstract',
    'openai_generated_abstract'
]
for col in text_columns:
    clean_col = col + "_clean"
    df[clean_col] = df[col].apply(preprocess_text)
print(" Preprocessing complete! Here are the new columns:")
print(df.columns)
df.head(5)

#Phase2.2

import matplotlib.pyplot as plt
from wordcloud import WordCloud
from collections import Counter
import seaborn as sns
import numpy as np

#Phase2.2

#Combine AI abstracts into one column

ai_texts = pd.concat([
    df['allam_generated_abstract_clean'],
    df['jais_generated_abstract_clean'],
    df['llama_generated_abstract_clean'],
    df['openai_generated_abstract_clean']
], axis=0).dropna().tolist()

#Phase2.2

human_texts = df['original_abstract_clean'].dropna().tolist()

#Phase2.2 #1

def text_stats(texts):
    words = [w for txt in texts for w in txt.split()]
    avg_word_len = np.mean([len(w) for w in words])
    avg_sent_len = np.mean([len(txt.split()) for txt in texts])
    vocab = set(words)
    ttr = len(vocab) / len(words)
    return avg_word_len, avg_sent_len, ttr

stats_human = text_stats(human_texts)
stats_ai = text_stats(ai_texts)

print("\n Statistical Summary:")
print(f"Human-written: Avg word len={stats_human[0]:.2f}, Avg sent len={stats_human[1]:.2f}, TTR={stats_human[2]:.3f}")
print(f"AI-generated : Avg word len={stats_ai[0]:.2f}, Avg sent len={stats_ai[1]:.2f}, TTR={stats_ai[2]:.3f}")

#Phase2.2 #1

import matplotlib.pyplot as plt

df["human_length"] = df["original_abstract"].apply(lambda x: len(x.split()))
df["ai_length"] = df["openai_generated_abstract"].apply(lambda x: len(x.split()))

plt.figure(figsize=(8,5))
plt.hist(df["human_length"], bins=30, alpha=0.6, label="Human-written", color='blue')
plt.hist(df["ai_length"], bins=30, alpha=0.6, label="AI-generated", color='orange')
plt.xlabel("Sentence Length (words)")
plt.ylabel("Frequency")
plt.title("Sentence Length Distribution")
plt.legend()
plt.show()

#Phase2.2 #1

def type_token_ratio(text):
    words = text.split()
    return len(set(words)) / len(words) if words else 0

df["human_ttr"] = df["original_abstract"].apply(type_token_ratio)
df["ai_ttr"] = df["openai_generated_abstract"].apply(type_token_ratio)

plt.figure(figsize=(6,5))
plt.boxplot([df["human_ttr"], df["ai_ttr"]], labels=["Human", "AI"])
plt.title("Vocabulary Richness (Typeâ€“Token Ratio)")
plt.ylabel("TTR Score")
plt.show()

#Phase2.2 #2

def plot_top_ngrams(texts, n=2, top_k=30):
    from sklearn.feature_extraction.text import CountVectorizer
    vec = CountVectorizer(ngram_range=(n, n))
    bag = vec.fit_transform(texts)
    sum_words = bag.sum(axis=0)
    freqs = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    freqs = sorted(freqs, key=lambda x: x[1], reverse=True)[:top_k]
    words, counts = zip(*freqs)
    plt.figure(figsize=(10,4))
    sns.barplot(x=list(counts), y=list(words))
    plt.title(f"Top {top_k} {n}-grams â€“ {n}-grams for {'Human' if texts==human_texts else 'AI'} abstracts")
    plt.show()

print("\nðŸ”¤ Top Bigrams for Human-written abstracts:")
plot_top_ngrams(human_texts, n=2)

print("\nðŸ”¤ Top Bigrams for AI-generated abstracts:")
plot_top_ngrams(ai_texts, n=2)

#Phase2.2 #3

from collections import Counter
import pandas as pd

human_words = " ".join(df["original_abstract"]).split()
ai_words = " ".join(df["openai_generated_abstract"]).split()

human_freq = Counter(human_words)
ai_freq = Counter(ai_words)

common_words = set(list(human_freq.keys())[:100]) & set(list(ai_freq.keys())[:100])

data = []
for w in common_words:
    data.append((w, human_freq[w], ai_freq[w]))

freq_df = pd.DataFrame(data, columns=["word", "human", "ai"]).sort_values("human", ascending=False)[:15]

freq_df.plot(x="word", kind="bar", figsize=(10,5), title="Top Words: Human vs AI", rot=45)
plt.ylabel("Frequency")
plt.show()